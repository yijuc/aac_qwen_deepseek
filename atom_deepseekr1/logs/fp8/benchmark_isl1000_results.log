
========================================
Running benchmark:
  Input tokens: 1000
  Output tokens: 1000
  Max concurrency: 4
  Num prompts: 40
  Resquest rate: inf
  Started at: Tue Dec 16 07:16:37 UTC 2025
========================================
Namespace(backend='vllm', base_url='http://localhost:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=4, model='/shared/amdgpu/home/share/deepseek/DeepSeek-R1', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=40, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=True, metadata=None, result_dir='logs', result_filename='deepseek_r1_FP8_tp8_isl1000_osl1000_conc4_infrrate.json', ignore_eos=True, percentile_metrics='ttft,tpot,itl,e2el', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1000, random_output_len=1000, random_range_ratio=1.0, random_prefix_len=0, use_chat_template=False, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 4
  0%|          | 0/40 [00:00<?, ?it/s]  2%|▎         | 1/40 [00:11<07:13, 11.12s/it] 12%|█▎        | 5/40 [00:22<02:20,  4.02s/it] 22%|██▎       | 9/40 [00:33<01:43,  3.33s/it] 32%|███▎      | 13/40 [00:44<01:23,  3.09s/it] 42%|████▎     | 17/40 [00:55<01:08,  2.97s/it] 52%|█████▎    | 21/40 [01:06<00:55,  2.90s/it] 62%|██████▎   | 25/40 [01:17<00:42,  2.86s/it] 72%|███████▎  | 29/40 [01:28<00:31,  2.83s/it] 82%|████████▎ | 33/40 [01:39<00:19,  2.82s/it] 92%|█████████▎| 37/40 [01:51<00:08,  2.80s/it]100%|██████████| 40/40 [01:51<00:00,  2.78s/it]
============ Serving Benchmark Result ============
Successful requests:                     40        
Benchmark duration (s):                  111.03    
Total input tokens:                      40000     
Total generated tokens:                  40000     
Request throughput (req/s):              0.36      
Output token throughput (tok/s):         360.26    
Total Token throughput (tok/s):          720.52    
---------------Time to First Token----------------
Mean TTFT (ms):                          140.72    
Median TTFT (ms):                        154.98    
P99 TTFT (ms):                           170.36    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          10.97     
Median TPOT (ms):                        10.97     
P99 TPOT (ms):                           11.04     
---------------Inter-token Latency----------------
Mean ITL (ms):                           10.96     
Median ITL (ms):                         10.94     
P99 ITL (ms):                            11.47     
----------------End-to-end Latency----------------
Mean E2EL (ms):                          11101.81  
Median E2EL (ms):                        11112.81  
P99 E2EL (ms):                           11127.23  
==================================================

Completed at: Tue Dec 16 07:18:41 UTC 2025
========================================

========================================
Running benchmark:
  Input tokens: 1000
  Output tokens: 1000
  Max concurrency: 8
  Num prompts: 80
  Resquest rate: inf
  Started at: Tue Dec 16 07:18:44 UTC 2025
========================================
Namespace(backend='vllm', base_url='http://localhost:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=8, model='/shared/amdgpu/home/share/deepseek/DeepSeek-R1', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=80, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=True, metadata=None, result_dir='logs', result_filename='deepseek_r1_FP8_tp8_isl1000_osl1000_conc8_infrrate.json', ignore_eos=True, percentile_metrics='ttft,tpot,itl,e2el', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1000, random_output_len=1000, random_range_ratio=1.0, random_prefix_len=0, use_chat_template=False, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 8
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:12<16:04, 12.21s/it] 11%|█▏        | 9/80 [00:24<02:48,  2.37s/it] 21%|██▏       | 17/80 [00:36<01:58,  1.89s/it] 31%|███▏      | 25/80 [00:48<01:35,  1.73s/it] 41%|████▏     | 33/80 [01:00<01:17,  1.65s/it] 51%|█████▏    | 41/80 [01:13<01:02,  1.60s/it] 61%|██████▏   | 49/80 [01:25<00:49,  1.58s/it] 71%|███████▏  | 57/80 [01:37<00:36,  1.57s/it] 81%|████████▏ | 65/80 [01:49<00:23,  1.55s/it] 91%|█████████▏| 73/80 [02:01<00:10,  1.54s/it]100%|██████████| 80/80 [02:01<00:00,  1.52s/it]
============ Serving Benchmark Result ============
Successful requests:                     80        
Benchmark duration (s):                  121.92    
Total input tokens:                      80000     
Total generated tokens:                  80000     
Request throughput (req/s):              0.66      
Output token throughput (tok/s):         656.19    
Total Token throughput (tok/s):          1312.39   
---------------Time to First Token----------------
Mean TTFT (ms):                          247.90    
Median TTFT (ms):                        264.41    
P99 TTFT (ms):                           275.72    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          11.95     
Median TPOT (ms):                        11.93     
P99 TPOT (ms):                           12.19     
---------------Inter-token Latency----------------
Mean ITL (ms):                           11.94     
Median ITL (ms):                         11.90     
P99 ITL (ms):                            12.85     
----------------End-to-end Latency----------------
Mean E2EL (ms):                          12189.35  
Median E2EL (ms):                        12185.00  
P99 E2EL (ms):                           12390.59  
==================================================

Completed at: Tue Dec 16 07:20:59 UTC 2025
========================================

========================================
Running benchmark:
  Input tokens: 1000
  Output tokens: 1000
  Max concurrency: 16
  Num prompts: 160
  Resquest rate: inf
  Started at: Tue Dec 16 07:21:02 UTC 2025
========================================
Namespace(backend='vllm', base_url='http://localhost:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=16, model='/shared/amdgpu/home/share/deepseek/DeepSeek-R1', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=160, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=True, metadata=None, result_dir='logs', result_filename='deepseek_r1_FP8_tp8_isl1000_osl1000_conc16_infrrate.json', ignore_eos=True, percentile_metrics='ttft,tpot,itl,e2el', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1000, random_output_len=1000, random_range_ratio=1.0, random_prefix_len=0, use_chat_template=False, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 16
  0%|          | 0/160 [00:00<?, ?it/s]  1%|          | 1/160 [00:13<35:39, 13.46s/it] 11%|█         | 17/160 [00:27<03:19,  1.40s/it] 21%|██        | 33/160 [00:41<02:18,  1.09s/it] 31%|███       | 49/160 [00:54<01:49,  1.02it/s] 31%|███▏      | 50/160 [00:55<01:46,  1.03it/s] 41%|████      | 65/160 [01:08<01:27,  1.09it/s] 41%|████▏     | 66/160 [01:08<01:25,  1.10it/s] 51%|█████     | 81/160 [01:22<01:10,  1.12it/s] 51%|█████▏    | 82/160 [01:22<01:08,  1.13it/s] 61%|██████    | 97/160 [01:35<00:54,  1.15it/s] 61%|██████▏   | 98/160 [01:36<00:53,  1.16it/s] 71%|███████   | 113/160 [01:49<00:40,  1.16it/s] 71%|███████▏  | 114/160 [01:49<00:39,  1.17it/s] 81%|████████  | 129/160 [02:02<00:26,  1.18it/s] 81%|████████▏ | 130/160 [02:03<00:25,  1.19it/s] 91%|█████████ | 145/160 [02:16<00:12,  1.17it/s] 91%|█████████▏| 146/160 [02:17<00:11,  1.18it/s]100%|██████████| 160/160 [02:17<00:00,  1.17it/s]
============ Serving Benchmark Result ============
Successful requests:                     160       
Benchmark duration (s):                  137.06    
Total input tokens:                      160000    
Total generated tokens:                  160000    
Request throughput (req/s):              1.17      
Output token throughput (tok/s):         1167.37   
Total Token throughput (tok/s):          2334.73   
---------------Time to First Token----------------
Mean TTFT (ms):                          427.53    
Median TTFT (ms):                        435.27    
P99 TTFT (ms):                           850.26    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          13.28     
Median TPOT (ms):                        13.34     
P99 TPOT (ms):                           13.73     
---------------Inter-token Latency----------------
Mean ITL (ms):                           13.27     
Median ITL (ms):                         13.18     
P99 ITL (ms):                            15.04     
----------------End-to-end Latency----------------
Mean E2EL (ms):                          13698.98  
Median E2EL (ms):                        13726.48  
P99 E2EL (ms):                           14330.62  
==================================================

Completed at: Tue Dec 16 07:23:33 UTC 2025
========================================

========================================
Running benchmark:
  Input tokens: 1000
  Output tokens: 1000
  Max concurrency: 32
  Num prompts: 320
  Resquest rate: inf
  Started at: Tue Dec 16 07:23:36 UTC 2025
========================================
Namespace(backend='vllm', base_url='http://localhost:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=32, model='/shared/amdgpu/home/share/deepseek/DeepSeek-R1', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=320, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=True, metadata=None, result_dir='logs', result_filename='deepseek_r1_FP8_tp8_isl1000_osl1000_conc32_infrrate.json', ignore_eos=True, percentile_metrics='ttft,tpot,itl,e2el', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1000, random_output_len=1000, random_range_ratio=1.0, random_prefix_len=0, use_chat_template=False, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 32
  0%|          | 0/320 [00:00<?, ?it/s]  0%|          | 1/320 [00:17<1:32:46, 17.45s/it] 10%|█         | 33/320 [00:34<04:20,  1.10it/s]  20%|██        | 65/320 [00:52<02:57,  1.44it/s] 30%|███       | 97/320 [01:09<02:19,  1.60it/s] 40%|████      | 129/320 [01:27<01:53,  1.68it/s] 50%|█████     | 161/320 [01:44<01:31,  1.74it/s] 60%|██████    | 193/320 [02:02<01:11,  1.77it/s] 70%|███████   | 225/320 [02:19<00:53,  1.78it/s] 80%|████████  | 257/320 [02:37<00:35,  1.80it/s] 90%|█████████ | 289/320 [02:54<00:17,  1.80it/s]100%|██████████| 320/320 [02:54<00:00,  1.83it/s]
============ Serving Benchmark Result ============
Successful requests:                     320       
Benchmark duration (s):                  174.75    
Total input tokens:                      320000    
Total generated tokens:                  320000    
Request throughput (req/s):              1.83      
Output token throughput (tok/s):         1831.23   
Total Token throughput (tok/s):          3662.47   
---------------Time to First Token----------------
Mean TTFT (ms):                          676.28    
Median TTFT (ms):                        704.67    
P99 TTFT (ms):                           910.81    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.81     
Median TPOT (ms):                        16.82     
P99 TPOT (ms):                           17.36     
---------------Inter-token Latency----------------
Mean ITL (ms):                           16.79     
Median ITL (ms):                         16.59     
P99 ITL (ms):                            18.18     
----------------End-to-end Latency----------------
Mean E2EL (ms):                          17468.94  
Median E2EL (ms):                        17475.97  
P99 E2EL (ms):                           17608.98  
==================================================

Completed at: Tue Dec 16 07:26:44 UTC 2025
========================================

========================================
Running benchmark:
  Input tokens: 1000
  Output tokens: 1000
  Max concurrency: 64
  Num prompts: 640
  Resquest rate: inf
  Started at: Tue Dec 16 07:26:47 UTC 2025
========================================
Namespace(backend='vllm', base_url='http://localhost:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=64, model='/shared/amdgpu/home/share/deepseek/DeepSeek-R1', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=640, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=True, metadata=None, result_dir='logs', result_filename='deepseek_r1_FP8_tp8_isl1000_osl1000_conc64_infrrate.json', ignore_eos=True, percentile_metrics='ttft,tpot,itl,e2el', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1000, random_output_len=1000, random_range_ratio=1.0, random_prefix_len=0, use_chat_template=False, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 64
  0%|          | 0/640 [00:00<?, ?it/s]  0%|          | 1/640 [00:20<3:40:41, 20.72s/it] 10%|█         | 65/640 [00:41<05:12,  1.84it/s]  20%|██        | 129/640 [01:01<03:30,  2.43it/s] 30%|███       | 193/640 [01:21<02:44,  2.72it/s] 40%|████      | 257/640 [01:42<02:14,  2.86it/s] 50%|█████     | 321/640 [02:02<01:47,  2.97it/s] 60%|██████    | 385/640 [02:23<01:24,  3.02it/s] 70%|███████   | 449/640 [02:43<01:02,  3.05it/s] 80%|████████  | 513/640 [03:04<00:41,  3.08it/s] 90%|█████████ | 577/640 [03:24<00:20,  3.10it/s]100%|██████████| 640/640 [03:24<00:00,  3.13it/s]
============ Serving Benchmark Result ============
Successful requests:                     640       
Benchmark duration (s):                  204.37    
Total input tokens:                      640000    
Total generated tokens:                  640000    
Request throughput (req/s):              3.13      
Output token throughput (tok/s):         3131.51   
Total Token throughput (tok/s):          6263.03   
---------------Time to First Token----------------
Mean TTFT (ms):                          1161.67   
Median TTFT (ms):                        1218.39   
P99 TTFT (ms):                           1766.19   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.28     
Median TPOT (ms):                        19.18     
P99 TPOT (ms):                           20.33     
---------------Inter-token Latency----------------
Mean ITL (ms):                           19.26     
Median ITL (ms):                         18.65     
P99 ITL (ms):                            21.57     
----------------End-to-end Latency----------------
Mean E2EL (ms):                          20426.28  
Median E2EL (ms):                        20443.48  
P99 E2EL (ms):                           20712.23  
==================================================

Completed at: Tue Dec 16 07:30:26 UTC 2025
========================================

========================================
Running benchmark:
  Input tokens: 1000
  Output tokens: 1000
  Max concurrency: 128
  Num prompts: 1280
  Resquest rate: inf
  Started at: Tue Dec 16 07:30:29 UTC 2025
========================================
Namespace(backend='vllm', base_url='http://localhost:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=128, model='/shared/amdgpu/home/share/deepseek/DeepSeek-R1', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1280, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=True, metadata=None, result_dir='logs', result_filename='deepseek_r1_FP8_tp8_isl1000_osl1000_conc128_infrrate.json', ignore_eos=True, percentile_metrics='ttft,tpot,itl,e2el', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1000, random_output_len=1000, random_range_ratio=1.0, random_prefix_len=0, use_chat_template=False, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 128
  0%|          | 0/1280 [00:00<?, ?it/s]  0%|          | 1/1280 [00:26<9:28:47, 26.68s/it] 10%|█         | 129/1280 [00:53<06:45,  2.84it/s] 20%|██        | 257/1280 [01:19<04:30,  3.78it/s] 30%|███       | 385/1280 [01:45<03:32,  4.21it/s] 40%|████      | 513/1280 [02:11<02:52,  4.44it/s] 50%|█████     | 641/1280 [02:38<02:19,  4.58it/s] 60%|██████    | 769/1280 [03:04<01:49,  4.65it/s] 70%|███████   | 897/1280 [03:31<01:20,  4.73it/s] 80%|████████  | 1025/1280 [03:57<00:53,  4.77it/s] 90%|█████████ | 1153/1280 [04:23<00:26,  4.81it/s]100%|██████████| 1280/1280 [04:23<00:00,  4.86it/s]
============ Serving Benchmark Result ============
Successful requests:                     1280      
Benchmark duration (s):                  263.63    
Total input tokens:                      1280000   
Total generated tokens:                  1280000   
Request throughput (req/s):              4.86      
Output token throughput (tok/s):         4855.31   
Total Token throughput (tok/s):          9710.62   
---------------Time to First Token----------------
Mean TTFT (ms):                          2096.20   
Median TTFT (ms):                        2036.35   
P99 TTFT (ms):                           3685.50   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          24.27     
Median TPOT (ms):                        24.32     
P99 TPOT (ms):                           26.19     
---------------Inter-token Latency----------------
Mean ITL (ms):                           24.24     
Median ITL (ms):                         22.68     
P99 ITL (ms):                            27.98     
----------------End-to-end Latency----------------
Mean E2EL (ms):                          26340.78  
Median E2EL (ms):                        26338.43  
P99 E2EL (ms):                           26664.85  
==================================================

Completed at: Tue Dec 16 07:35:10 UTC 2025
========================================
